<!DOCTYPE html>
<html lang="en">
<title>Jiaxi He</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
<style>
body {
    font-family: "Helvetica Neue", sans-serif;
    margin: 0;
}
.mySlides {
    display: none
}
    .timeline{
  --uiTimelineMainColor: var(--timelineMainColor, #222);
  --uiTimelineSecondaryColor: var(--timelineSecondaryColor, #fff);

  position: relative;
  padding-top: 3rem;
  padding-bottom: 3rem;
}

.timeline:before{
  content: "";
  width: 4px;
  height: 100%;
  background-color: var(--uiTimelineMainColor);

  position: absolute;
  top: 0;
}

.timeline__group{
  position: relative;
}

.timeline__group:not(:first-of-type){
  margin-top: 4rem;
}

.timeline__year{
  padding: .5rem 1.5rem;
  color: var(--uiTimelineSecondaryColor);
  background-color: var(--uiTimelineMainColor);

  position: absolute;
  left: 0;
  top: 0;
}

.timeline__box{
  position: relative;
  /* position: top; */
}

.timeline__box:not(:last-of-type){
  margin-bottom: 30px;
}

.timeline__box:before{
  content: "";
  width: 100%;
  height: 2px;
  background-color: var(--uiTimelineMainColor);

  position: absolute;
  left: 0;
  z-index: -1;
}

.timeline__date{
  min-width: 65px;
  position: absolute;
  left: 0;

  box-sizing: border-box;
  padding: .5rem 1.5rem;
  text-align: center;

  background-color: var(--uiTimelineMainColor);
  color: var(--uiTimelineSecondaryColor);
}

.timeline__day{
  font-size: 2rem;
  font-weight: 700;
  display: block;
}

.timeline__month{
  display: block;
  font-size: .8em;
  text-transform: uppercase;
}

.timeline__post{
  padding: 1.5rem 2rem;
  border-radius: 2px;
  border-left: 3px solid var(--uiTimelineMainColor);
  box-shadow: 0 1px 3px 0 rgba(0, 0, 0, .12), 0 1px 2px 0 rgba(0, 0, 0, .24);
  background-color: var(--uiTimelineSecondaryColor);
}

@media screen and (min-width: 641px){

  .timeline:before{
    left: 30px;
  }

  .timeline__group{
    padding-top: 55px;
  }

  .timeline__box{
    padding-left: 80px;
  }

  .timeline__box:before{
    top: 50%;
    transform: translateY(-50%);
  }

  .timeline__date{
    top: 5%;
    margin-top: -0px;
  }
}

@media screen and (max-width: 640px){

  .timeline:before{
    left: 0;
  }

  .timeline__group{
    padding-top: 40px;
  }

  .timeline__box{
    padding-left: 20px;
    padding-top: 70px;
  }

  .timeline__box:before{
    top: 90px;
  }

  .timeline__date{
    top: 0;
  }
}

.timeline{
  /* --timelineMainColor: #4557bb; */
  --timelineMainColor: #4679bd;
  font-size: 15px;
}

/*
=====
DEMO
=====
*/

@media (min-width: 768px){

  html{
    font-size: 62.5%;
  }
}

@media (max-width: 767px){

  html{
    font-size: 55%;
  }
}

body{
  font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Open Sans, Ubuntu, Fira Sans, Helvetica Neue, sans-serif;
  font-size: 1.6rem;
  color: #222;

  background-color: #f0f0f0;
  margin: 0;
  -webkit-overflow-scrolling: touch;
  overflow-y: scroll;

  display: flex;
  flex-direction: column;
}

p{
  margin-top: 0;
  margin-bottom: 1.5rem;
  line-height: 1.5;
  text-align: justify;
}

p:last-child{
  margin-bottom: 0;
}

.page{
  max-width: 800px;
  padding: 10rem 2rem 3rem;
  margin-left: auto;
  margin-right: auto;
  order: 1;
}

/*
=====
LinkedIn
=====
*/

/* .linkedin{
  background-color: #fff;
  text-align: center;
}

.linkedin__container{
  max-width: 1000px;
  padding: 10px;
  margin-left: auto;
  margin-right: auto;
}

.linkedin__text{
  margin-top: 0;
  margin-bottom: 0;
}

.linkedin__link{
  color: #ff5c5c;
} */

h2 {
    margin-bottom: 30px;
    color: #4679bd;
    font-weight: 200;
    text-align: center;
}

</style>

<body>

    <div class="w3-content" style="max-width:2000px;bottom:0px;">
<!-- intro -->
    	<section class="w3-container w3-center w3-content" style="max-width:600px;">
    	</br>
    		<h1 class = "w3-wide w3-center" >Jiaxi He</h1>
    		<h3 class = "w3-opacity w3-center">Passion. Professional. Trust.</h3>
    	</br>
    		<p class = "w3-justify w3-center">I am a climber on the route to business intelligence.</p>
    		<p class = "w3-justify w3-center">I spot values and oppotunities from data.</p>
    		<p class = "w3-justify w3-center">I consult, share knowledge, and help clients to grow.</p>
    	</section>
    <br>
    
<!-- showcases -->
            <section class="w3-container w3-padding-16 w3-center w3-content w3-light-grey">
            <h3 class = " w3-center">Showcases</h3>
            <article class ="w3-half">
                <img src="https://www.nationalgallery.sg/sites/default/files/crawl_images/P-0320/P_0320_(cropped)_20160215122753_1080p.jpg" height="120" ></img>
                <p align ='left'><b>The "Artlogue" System</b></p>
                <p align ='left' style="max-width:450px;">A dialogue interface to provide customized guided tour via natural language in accordance with the taste of an audience. A shared vision to build conversational AI in defined tasks between NUS and the National Gallery of Singapore.</p>
            </article>
            <article class ="w3-half">
                <img src="https://upload.wikimedia.org/wikipedia/commons/0/0e/Lobes_of_the_brain_NL.svg" height="120"></img>
                <p align ='left'><b>The "Cerebrum" Engine</b></p>
                <p align ='left' style="max-width:450px;">A platform for text analysis, featuring classification, semantic matching, information extraction, risk spotting, compliance, legal document review etc. Read more details from its <a href="https://www2.deloitte.com/cn/zh/pages/risk/articles/deloitte-cerebrum-engine-data-governance.html">introduction</a>, <a href="https://www2.deloitte.com/cn/zh/pages/risk/articles/deloitte-cerebrum-deep-machine-learning.html">architecture</a>, and <a href="https://www2.deloitte.com/cn/zh/pages/risk/articles/deloitte-intelligent-engine.html">applications</a>.</p>
            </article>
        </section>
    <br>

  <!-- Teaching -->
    <section class="w3-container w3-padding-16 w3-center w3-content w3-light-grey">
    <h2 class = " w3-center">Tutorials</h2>
        <p align ='left'><b>An Introductory Course on Deep Learning with Keras</b></p>
    <article class ="w3-half">
      <iframe width="450" height="315" src="https://www.youtube.com/embed/sBUyl8BBh8A" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <p style ="text-align:center">Part 1: Deep Learning Foundations</p>
      <p style ="text-align:left">This video talks about the foundations of deep learning. When you finish this video, you will understand the major components of Deep Neural Networks, like perceptron, activation functions, forward and backpropagation, as well as the optimizers.</p>
    </article>
    <article class ="w3-half">
      <iframe width="450" height="315" src="https://www.youtube.com/embed/ZfIX7vyfl3w" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe
      <p style ="text-align:center">Part 2: Deep Learning with Keras</p>
      <p style ="text-align:left">In this 1-hour video, you will be able to build and train a fully connected deep neural network in Keras for the classic digital handwritten problem. You will understand the key parameters in a neural network's architecture. </p>
        <!-- It also teaches you how to improve the performance of your neural networks, like hyperparameter tuning and regularization.  -->
    </article>
    </section>
    <br>   

    <!-- Memos -->
    <section class="w3-container w3-padding-32 w3-center w3-content w3-light-grey" >
      <h2 class = " w3-center">Memos</h2>
      <div class="page" style="max-height:1200px;overflow:scroll;">
        <div class="timeline">
          <div class="timeline__group">
            <span class="timeline__year">2020</span>
              
            <!--2020 June 25  -->
            <div class="timeline__box">
              <div class="timeline__date">
                <span class="timeline__day">25</span>
                <span class="timeline__month">June</span>
              </div>
              <div class="timeline__post">
                <div class="timeline__content">
                  <p>AdaX: a optimizer with "long-term memory" and performs better than Adam</p>
                </div>
                <hr style="height:1px;border-width:0;color:gray;background-color:gray">
                <div class="timeline__content">
                  <p><b>Text generation metrics </b></p>
                  <p>1. Traditional metrics based on “n-gram reduplication“ like BLEU and Rouge is still used widely </p>
                  <p>2. By adding short sentence penalty, key information loss penalty， coverage and diversity concern, metrics like METEOR, SPICE, Distinct are derived.</p>
                  <p>3. PPL, Bertscore[1], which based on language models, are getting hit recently.</p>
                  <p>[1]. Bertscore: evaluating text generation with BERT, ICLR 2020 .</p>
                </div>
              </div>
            </div>
              
            <!--2020 June 18  -->
            <div class="timeline__box">
              <div class="timeline__date">
                <span class="timeline__day">18</span>
                <span class="timeline__month">June</span>
              </div>
              <div class="timeline__post">
                <div class="timeline__content">
                  <p><b>Research Areas:</b></p>
                  <p>1.Chit-chat bot: Generative; Retrieval</p>
                  <p>2.Question-Answering bot: Retrieval; FAQ-based; KB-based; reading comprehension (SQuAD) </p>
                  <p>3.Task-oriented bot: Strategic reinforcement learning; lack human dialogue dataset; SOTA-one sentence only contain one intent. </p>
                  <p>4.Knowledge Base: KG embedding (Open KE by Tsinghua) -> KG Completion -> Relation extraction ->QA  </p>
                </div>
                <hr style="height:1px;border-width:0;color:gray;background-color:gray">
                <div class="timeline__content">
                  <p><b>NLP task Dataset: </b></p>
                  <p>2016: SQuAD </p>
                  <p>2018: GLUE (the combination of 8 nlp tasks [1]), CoQA</p>
                  <p>2019: SuperGLUE, MRQA</p>
                  <p>2020: MuTual: Multi-Turn Dialogue Reasoning [2]. The China English test of dialogue and multi-choices.</p>
                  <p>[1]. https://zhuanlan.zhihu.com/p/135283598</p>
                  <p>[2]. https://zhuanlan.zhihu.com/p/129684559</p>
                </div>
              </div>
            </div>
              
            <!--2020 June 12  -->
            <div class="timeline__box">
              <div class="timeline__date">
                <span class="timeline__day">13</span>
                <span class="timeline__month">June</span>
              </div>
              <div class="timeline__post">
                <div class="timeline__content">
                  <h3>Dialogue system review</h3>
                  <p>The dialogue system can usually be divided into three categories based on the user's requirement: <i>the "chit-chat" bot</i>; <i>the task-oriented bot</i>; and <i>the knowledge-inquiring (question-answering) bot</i>. These three types are not mutually exclusive, and there are combined scenarios, like a virtual docent can be both a chit-chat and a knowledge-inquiring bot. Apart from the above, we can also classify the dialogue system into a <i>single-turn</i> and <i>multi-turn</i> conversational bot, depending on whether to consider the conversation context information. This article starts with the "chit-chat" bot.</p>
                  <p><b>The "chit-chat" bot:</b> can use 1. generative-baed approach, 2. retrieval-based approach, or 3. combined approach.</p>
                  <img src="dialogue.png" height="300" ></img>
                  <figcaption>Fig.1 - Dialogue system.</figcaption>
                  <br>
                  <p>1. Generative-based approach</p>
                  <p> The approach can be regarded as a seq2seq model, which obtains great success in the machine translation problem. However, machine translation is more like an objective problem with objective criteria/metrics; while, the dialogue generation is more like a subjective problem with no standard answer [2]. Therefore the assessment for dialogue generation is a troublesome problem, there are a few research works on the dialogue assessment like ADEM and RUBER. Generally, the dialogue generation is a conditional generation problem that requires high coupling between query and answers.</p>
                  <p>Due to its low reply quality and uncontrollable, the industry prefers to retrieval-based approach that is more practical and robust.</p>
                  <p>2. Retrieval-based approach </p>
                  <img src="retrieval.jpg" width="600" ></img>
                  <figcaption>Fig.2 - Retrieval-based conversation approach.</figcaption>
                  <br>
                  <p>The retrieval-based approach studies the response selection. It is suitable for both chitchat and knowledge-inquiring/QA tasks. For QA/FAQ knowledge-inquiring task, single-turn conversation dominates most cases. However, the chit-chat is more incline to multi-turn and context-based. For the dataset, the FAQ is usually closed-domain, and it is easy to get, for example, the corporate documents; while the chit-chat usually has no scope limit and is open-domain, thus the query-response pairs corpus is needed, that is we need to crawl the dialogue corpus from public social media like Twitter, Facebook, and Instagram. </p>
                  <p>The core of the retrieval-based approach is to construct the (query-response/context-query-response) matching. Some people may found confused when reading research papers about relevance matching and semantic marching [1]. The relevance matching is to rank documents by relevance to a user's query. The semantic matching is used to measure the semantic distance between two pieces of short texts. In other words, the relevance matching is fundamentally an information retrieval (IR) matching task, it operates directly on the similarity matrix obtained from products of query and document embeddings and builds modules on top to capture additional n-gram matching and term importance signals. On the other hand, semantic matching is more an NLP task, which requires more semantic understanding, contextual reasoning, and co-attention (context-aware) rather than specific term matches. In the dialogue system, semantic matching is a better choice. </p>
                  <img src="matching.png" width="600" ></img>
                  <figcaption>Fig.3 - Relevance matching vs semantic matching.</figcaption>
                  <br>
                  <p>2.1 single-turn conversation</p>
                  <p>For single-turn conversation, we ignore the conversation history and only focus on the query-response matching, the mainstream methods include semantic representation-oriented technique and semantic interaction-oriented technique. </p>
                  <p>2.1.1 representation-oriented technique</p>
                  <p>In the representation oriented framework, the deep learning technique is employed to focus on the mapping of objects from the original text space to the feature space, that is, to calculate the embeddings of query and documents that contain candidate responses.</p>
                  <img src="representation.png" width="600" ></img>
                  <figcaption>Fig.4 - Representation-oriented cconversation model.</figcaption>
                  <br>
                  <p>Currently, many contextual pre-trained sentence embedding models, such as Elmo, BERT (and its variants), GPT family, can be applied and fine-tuned to get a satisfying result.</p>
                  <p>2.1.2 interaction-oriented technique</p>
                  <p>The interaction-oriented technique model the semantic interaction structure between reply and query, focusing on capturing the meaningful interaction matching information from words, phrases, and sentences of reply and query. Fig.5 illustrates an example of interaction structure at different levels.</p>
                  <img src="interaction.png" width="600" ></img>
                  <figcaption>Fig.5 - interaction-oriented cconversation model..</figcaption>
                  <br>
                  <p>One of the classic interaction-oriented technique for conversation, I would like to introduce here, is the MatchPyramid model [4]. The overview of the MatchPyramid model is shown in the Figure below. Firstly, a matching matrix whose entries represent the similarities between words is constructed and viewed as an image. Then a convolutional neural network is utilized to capture rich matching patterns in a layer-by-layer way. By resembling the compositional hierarchies of patterns in image recognition, the MatchPyramid model can successfully identify salient signals such as n-gram and n-term matchings.</p>
                  <img src="matchpyramid.png" width="600" ></img>
                  <figcaption>Fig.6 - Match Pyramid interaction matching model.</figcaption>
                  <br>
                  <p>2.2 multi-turn conversation</p>
                  <p>The biggest difference between a single-turn and a multi-turn conversation model is that the multi-turn conversation model needs to integrate the current query and history conversation as input; its goal is to select a candidate response, which is relevant to the current query as well as corresponding to the conversation context, Fig.7. Still, the mainstream technique can be divided into representation-oriented and interaction-oriented.</p>
                  <img src="multi_conv.png" width="600" ></img>
                  <figcaption>Fig.7 - An example of multi-turn conversation.</figcaption>
                  <br>
                  <p>2.2.1 representation-oriented technique</p>
                  <p>Each conversation text is called an utterance. The diagram of the representation-oriented technique for multi-turn conversation is shown in Fig.8. For the utterance representation layer, pre-trained language models like Elmo, BERT, GPT can be applied to calculate the embeddings. For the context representation layer, a simple way is using the pooling technique; autoencoder, DNN, and RNN can also be used to learn the context representation layer from sentence embeddings if you have enough duplicate conversation corpus.  Apart from matching a response with a highly abstract context vector, other existing work may concatenate utterances in context, however, these representation-oriented approaches prone to lose relationships among utterances or important contextual information, thus we can turn to the interaction-oriented technique. </p>
                  <img src="multi.png" width="600" ></img>
                  <figcaption>Fig.8 - Diagram for retieval-based representation-oriented multi-turn conversation model.</figcaption>
                  <br>
                  <p>2.2.2 interaction-oriented technique</p>
                  <p>Compared to the representation-oriented technique, the interaction-oriented technique models the interaction pattern between the candidate response and each utterance, hence this approach calculates the matching score in a finer semantic granularity, refer to Fig.9.</p>
                  <img src="interaction-multi.png" width="600" ></img>
                  <figcaption>Fig.9 - Diagram for retieval-based interaction-oriented multi-turn conversation model.</figcaption>
                  <br>
                  <p>Paper [5] proposed a sequential matching network (SMN), which is motivated by the Match Pyramid model. SMN first matches a response with each utterance in the context on multiple levels of granularity and distills important matching information from each pair as a vector with convolution and pooling operations. The vectors are then accumulated in chronological order through an RNN which models relationships among utterances. The final matching score is calculated with the hidden states of the RNN.</p>
                  <p>2.2.3 Dataset for multi-turn conversation</p>
                  <p>-Ubuntu Dialogue Corpus (English)</p>
                  <p>-Douban Conversation Corpus (Chinese)</p>
                  <p>Till now (2020 June), the representation-oriented technique using pre-trained language model BERT achieves the best performance on Ubuntu Dialogue Corpus [6].</p>
                  <p>2.2.4 Metrics </p>
                  <p>R2@1, R10@1, R10@2, R10@5.</p>
                  <br>
                  <p><b>The knowledge-inquiring/question-answering bot:</b></p>
                  <p>1. FAQ single conversation model</p>
                  <p>2. CoQA benchmark</p>
                  <p>3. KB-dialogue study</p>
                </div>
                <hr style="height:1px;border-width:0;color:gray;background-color:gray">
                <div class="timeline__content">
                  <p> </p>
                  <p> </p>
                  <p>[1]. Bridging the Gap Between Relevance Matching and Semantic Matching for Short Text Similarity Modeling.</p>
                  <p>[2]. https://zhuanlan.zhihu.com/p/83825070</p>
                  <p>[3]. Survey on deep learning based open domain dialogue system, chinese journal of computers.</p>
                  <p>[4]. Text Matching as Image Recognition, AAAI 2016. </p>
                  <p>[5]. Sequential Matching Network, arxiv 2017. </p>
                  <p>[6]. Domain Adaptive Training BERT for Response Selection. </p>
                </div>
              </div>
            </div>  
              
            <!--2020 June 2  -->
            <div class="timeline__box">
              <div class="timeline__date">
                <span class="timeline__day">2</span>
                <span class="timeline__month">June</span>
              </div>
              <div class="timeline__post">
                <div class="timeline__content">
                  <p>Semantic search using Siamese BERT Networks: </p>
                  <p>1. ELMO: feature-based model </p>
                  <p>2. BERT-AS-A-BERT body by Tencent AI: using the second-to-last embeddings from BERT and then use Average-pooling to derive a fixed-sized sentence embedding vector, the result is also not very satisfying. </p>
                  <p>3. <span style="color:red"> Sentence-BERT</span>: Siamese BERT network (2019 EMNLP) </p>
                  <p> Github: <url>https://github.com/UKPLab/sentence-transformers#Training </url> </p>
                  <p> BERT / RoBERTa / XLM-RoBERTa produces out-of-the-box rather bad sentence embeddings. This repository fine-tunes BERT / RoBERTa / DistilBERT / ALBERT / XLNet with a siamese or triplet network structure and adds a pooling (default:MEAN pooling) operation to the output, to produce semantically meaningful sentence embeddings that can be used in unsupervised scenarios: Semantic textual similarity via cosine-similarity, clustering, semantic search. </p>
                </div>
                <hr style="height:1px;border-width:0;color:gray;background-color:gray">
                <div class="timeline__content">
                  <p>Vector similarity caculation </p>
                  <p>Apart from the cosine-similarity, facebook AI proposed a "<span style="color:red">Faiss</span>" library, which wrapped in python/numpy, for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM.  </p>
                  <p> </p>
                </div>
              </div>
            </div>

            <!--2020 June 1  -->
            <div class="timeline__box">
              <div class="timeline__date">
                <span class="timeline__day">1</span>
                <span class="timeline__month">June</span>
              </div>
              <div class="timeline__post">
                <div class="timeline__content">
                  <p> Report "Semantic matching in search 2014" by Huawei (Click-through Data)：</p>
                  <p>1. Matching by query reformulation </p>
                  <p>-spelling error correction, merging, splitting, stemming... to improve the matching</p>
                  <p>2. Matching with Team Dependency Model</p>
                  <p>- multiple terms (phrases, n-grams, etc). Ex: "Hot dog" vs "Hot", "Dog"</p>
                  <p>3. Matching with Translation model</p>
                  <p>- Statistical machine translation technology can be used to deal with query and document mismatch in search "nyc"="new york city"</p>
                  <p>4. Matching with topic model</p>
                  <p>5. Matching with latent space model (Embedding)</p>
                  <p>- Nowadays, most frequently used</p>
                  <p>For the Arthena project, currently, it relies mainly on the fourth point to find the query and the embedding of the document, and then calculate the cosine similarity. We can think about how to improve/refine the chatbot using the remaining four techniques.</p>
                </div>
              </div>
            </div>

            <!--2020 May 28  -->
            <div class="timeline__box">
              <div class="timeline__date">
                <span class="timeline__day">28</span>
                <span class="timeline__month">May</span>
              </div>
              <div class="timeline__post">
                <div class="timeline__content">
                  <p>For the Arthena chatbot, if substitute the feature-based google nnlm-128 model to Bert base model and use the [cls] pre-trained embeddings, the semantic matching result is not good.</p>
                  <p>I may need to find a “similar sentences” dataset to fine-tune the Bert model.</p>
                </div>
              </div>
            </div>
          </div>
          <!-- -->
        </div>
      </div>
<!-- <div class="linkedin">
  <div class="linkedin__container">
    <p class="linkedin__text">I'm looking for my fans which I'll give away everything I know. If you're interested then 👉 <a href="https://www.patreon.com/posts/introduction-to-34774556" class="linkedin__link" rel="noopener noreferrer" target="_blank">join to me on Patreon!</a></p>
  </div>
</div> -->
   </section>
        
        <canvas class="w3-center" style="margin-bottom:0px;padding-bottom:0px;position:absolute;top:0px;bottom:0px;z-index:-1;">
        	<script src="local.js"></script>
        </canvas>
     </div>

<footer class="w3-container w3-padding-20 w3-center w3-black w3-xlarge">
  <a href="/cvhjx.pdf"><i class="fa fa-file-pdf-o w3-hover-opacity"></i></a>&nbsp;&nbsp;
  <a href="http://github.com/JiaxiHe"><i class="fa fa-github w3-hover-opacity"></i></a>&nbsp;&nbsp;
  <a href="http://www.researchgate.net/profile/jiaxi_he4"><i class="ai ai-researchgate-square w3-hover-opacity"></i></a>&nbsp;&nbsp;
  <a href="http://www.linkedin.com/in/jiaxi-he-20423747"><i class="fa fa-linkedin w3-hover-opacity"></i></a>&nbsp;&nbsp;
  <a href="http://www.facebook.com/jiaxi.he"><i class="fa fa-facebook-official w3-hover-opacity"></i></a>&nbsp;&nbsp;
  <a href="mailto:jiaxihe1989@gmail.com"><i class="fa fa-envelope-o w3-hover-opacity"></i></a>&nbsp;&nbsp;
  <p class="w3-medium">Modified by Jiaxi from w3.css</p>
</footer>

     <footer class="w3-container w3-padding-32 w3-center w3-black w3-xlarge w3-bottom">
     	<a href="/cvhjx.pdf"><i class="fa fa-file-pdf-o w3-hover-opacity"></i></a>&nbsp;&nbsp;
     	<a href="http://github.com/JiaxiHe"><i class="fa fa-github w3-hover-opacity"></i></a>&nbsp;&nbsp;
     	<a href="http://www.researchgate.net/profile/jiaxi_he4"><i class="ai ai-researchgate-square w3-hover-opacity"></i></a>&nbsp;&nbsp;
         <a href="http://www.linkedin.com/in/jiaxi-he-20423747"><i class="fa fa-linkedin w3-hover-opacity"></i></a>&nbsp;&nbsp;
         <a href="http://www.facebook.com/jiaxi.he"><i class="fa fa-facebook-official w3-hover-opacity"></i></a>&nbsp;&nbsp;
         <a href="mailto:jiaxihe1989@gmail.com"><i class="fa fa-envelope-o w3-hover-opacity"></i></a>&nbsp;&nbsp;
         <p style="text-align:center"  class="w3-medium">Modified by Jiaxi from w3.css</p>
     </footer>

</body>
</html>
